**开源评测集的模型性能不作为最终客观指标，但需计算其指标得分用于筛选是否进行闭源评测，具体筛选比例按实际有效报名人数动态确定。闭源评测集上的模型性能100%统计为客观指标最终得分**

## 开源测评集（门槛，100分，不计入总成绩）

&nbsp;&nbsp;&nbsp;&nbsp;该得分需在参赛作品“技术报告”中明确体现。禁止利用开源评测集进行微调后再进行评测。



### VRSBench（共50分）

#### 图像生成描述（25分）

&nbsp;&nbsp;&nbsp;&nbsp;给定一张遥感图像，模型需要生成描述图像内容的自然语言文本

##### 得分标准

- BLEU-1：计算生成文本与参考文本之间1-gram(单个词)的匹配度
- BLEU-2：计算2-gram(连续两个词)的匹配度
- BLEU-4：计算4-gram(连续四个词)的匹配度

&nbsp;&nbsp;&nbsp;&nbsp;最终取三个BLEU的平均值X1，得分=X1×25

#### 视觉定位（25分）

&nbsp;&nbsp;&nbsp;&nbsp;给定一张遥感图像和一个文本查询(如"定位图中的飞机")模型需要在图像中定位目标区域，输出一个边界框坐标

##### 得分标准

- 使用交并比评估定位准确性,$IOU=\frac{预测框与真实框的交集面积}{预测框与真实框的并集面积}$
- 定位成功标准：IoU ≥ 0.5
- 计算准确率X2 = 定位成功的样本数/总样本数

&nbsp;&nbsp;&nbsp;&nbsp;得分=X2×25

#### 视觉问答（50分）

&nbsp;&nbsp;&nbsp;&nbsp;开放式问题与多选择题 

##### 得分标准

- 开放式回答：模型输出文本必须完全匹配正确答案
- 多项选择题：模型输出选项字符必须完全匹配正确选项
- 计算$准确率X3=\frac{回答正确的样本数}{总样本数}$

&nbsp;&nbsp;&nbsp;&nbsp;得分=X3×50

计算出三部分总分除2



### MME RealWorld RS（50分）

&nbsp;&nbsp;&nbsp;&nbsp;只有多项选择题，计算$准确率X1=\frac{回答正确的样本数}{总样本数}$

&nbsp;&nbsp;&nbsp;&nbsp;得分=X1*100/2



## 参赛部分指标



### 闭源测评集（计入总分，占60分）

&nbsp;&nbsp;&nbsp;&nbsp;分为图像描述（15分）和视觉问答（45分），计算细节同VRSBench数据集

### 定性评分（40分）

&nbsp;&nbsp;&nbsp;&nbsp;对相关技术文档和代码复现进行打分评估

#### 系统实现与可复现性（20分）

- 代码支持在所提供的Docker中成功运行与复现，实现客观指标评估，代码执行过程需简单清晰（14分）；
- 实现遵循软件工程规范，模块划分清晰、接口定义明确，核心算法与数据处理流程进行专业化封装（2分）；
- 详细的Readme使用说明（4分）

#### 任务设计与数据组织（10分）

&nbsp;&nbsp;&nbsp;&nbsp;要求：技术方案可行，模型设计合理；任务目标明确，建模合理，评价指标科学；数据来源合规，预处理流程规范，具备代表性与质量控制，支撑训练与评估全过程。对应参赛作品形式为“技术报告”。

- 关键技术路线与实验设计阐述详实，配图完善（4分）；
- 数据构建方法简单明确，数据处理合理（3分）；
- 技术文档结构清晰，逻辑脉络清楚（3分）。

#### 创新性与应用价值（10分）

&nbsp;&nbsp;&nbsp;&nbsp;在模型结构、算法路径等方面具备创新性；方案可高效适配遥感任务，具备在城市规划、应急灾害、环境生态等场景中的落地潜力，本评分维度得分上限10分。对应参赛作品形式为“技术报告”与其他可选的参赛作品形式（如演示程序等）。

- 模型设计、训练算法、数据构建、模型推理方面存在创新性（6分）；
- 对最新技术有深度的思考与洞察力（2分）；
- 技术方案在特定应用场景可展示实际意义和价值（2分）。

### 附加分（10分）

&nbsp;&nbsp;&nbsp;&nbsp;利用可视化演示程序、交互式Web界面、演示视频展示模型或系统的运行效果，内容包含但不限于模型的输入输出，关键技术点和应用实例。